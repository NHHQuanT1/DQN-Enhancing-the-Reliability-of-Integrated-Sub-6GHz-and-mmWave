import environment as env
import save_result as save 
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
from collections import defaultdict

# Hyperparameters
NUM_DEVICES = 10  # S·ªë thi·∫øt b·ªã (K=3, scenario 1)
NUM_SUBCHANNELS = 16  # S·ªë subchannel Sub-6GHz (N)
NUM_BEAMS = 16  # S·ªë beam mmWave (M)
MAX_PACKETS = 6  # S·ªë g√≥i tin t·ªëi ƒëa m·ªói frame (L_k(t))
PLR_MAX = 0.1  # Gi·ªõi h·∫°n PLR t·ªëi ƒëa
GAMMA = 0.9  # Discount factor
EPS_START = 0.5  # Kh·ªüi ƒë·∫ßu epsilon
EPS_END = 0.05  # K·∫øt th√∫c epsilon
EPS_DECAY = 0.995  # Decay factor
BETA = -0.5
EPSILON = 1
NUM_OF_FRAME = 10000
T = 1e-3
D = 8000
I = 2  # S·ªë l∆∞·ª£ng Q-network
LAMBDA_P = 0.5
LAMBDA = 0.995
X0 = 1

# Chuy·ªÉn state, action th√†nh key
def state_action_to_key(state, action):
    """Chuy·ªÉn state v√† action th√†nh key ƒë·ªÉ l∆∞u trong dictionary"""
    state_key = tuple(map(tuple, state)) # chuy·ªÉn t·ª´ng h√†ng th√†nh 1 tuple, sau ƒë√≥ chuy·ªÉn th√†nh tuple l·ªìng tuple
    action_key = tuple(action) if isinstance(action, np.ndarray) else tuple(action) # ki·ªÉm tra xem c√≥ ph·∫£i array hay danh s√°ch (python) ƒë·ªÉ chuy·ªÉn v·ªÅ tuple c·∫£
    return (state_key, action_key)

# H√†m utility
def u(x):
    """H√†m utility"""
    return -np.exp(BETA * x)

# M√£ h√≥a action th√†nh index v√† ng∆∞·ª£c l·∫°i
def action_to_index(action):
    """Chuy·ªÉn action t·ª´ array sang index"""
    index = 0
    for i, a in enumerate(action): # Duy·ªát qua ph·∫ßn t·ª≠ a c·ªßa action ·ª©ng v·ªõi ch·ªâ s·ªë i, t·ª©c gi√° tr·ªã a[i] trong action truy·ªÅn v√†o
        index += int(a) * (3 ** i) # index = a + 3^i
    return index

def index_to_action(index):
    """Chuy·ªÉn index th√†nh action array"""
    action = np.zeros(NUM_DEVICES, dtype=int)
    for i in range(NUM_DEVICES):
        action[i] = index % 3
        index = index // 3
    return action

# ===== ƒê·ªãnh nghƒ©a ki·∫øn tr√∫c m·∫°ng neural network m·ªõi =====
class QNetwork(nn.Module):
    def __init__(self):
        super(QNetwork, self).__init__()
        self.state_dim = NUM_DEVICES * 4  # M·ªói thi·∫øt b·ªã c√≥ 4 ƒë·∫∑c tr∆∞ng
        self.action_size = 3**NUM_DEVICES  # T·ªïng s·ªë action (3 actions cho m·ªói thi·∫øt b·ªã)
        
        # X√¢y d·ª±ng m·∫°ng neural
        self.fc1 = nn.Linear(self.state_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        # ƒê·∫ßu ra c√≥ k√≠ch th∆∞·ªõc b·∫±ng s·ªë l∆∞·ª£ng action c√≥ th·ªÉ
        self.fc3 = nn.Linear(64, self.action_size)
        
    def forward(self, state):
        """
        Nh·∫≠n ƒë·∫ßu v√†o l√† state, tr·∫£ v·ªÅ Q-values cho t·∫•t c·∫£ action kh·∫£ thi
        """
        # Chuy·ªÉn state th√†nh tensor v√† l√†m ph·∫≥ng
        if not isinstance(state, torch.Tensor):
            state = torch.FloatTensor(state.flatten()).unsqueeze(0) #th√™m m·ªôt chi·ªÅu 
        
        x = F.relu(self.fc1(state)) #lan truy·ªÅn thu·∫≠n b·∫±ng h√†m ReLu
        x = F.relu(self.fc2(x))
        return self.fc3(x)
    
    def get_q_value(self, state, action):
        """L·∫•y Q-value cho m·ªôt action c·ª• th·ªÉ s·ª≠ d·ª•ng cho ƒë√°nh gi√°, ki·ªÉm tra"""
        with torch.no_grad(): #t·∫Øt t√≠nh to√°n gradient cho d·ª± ƒëo√°n
            q_values = self(state) #b·∫£n ch·∫•t  self.__call__(input)
            action_idx = action_to_index(action)
            return q_values[0, action_idx].item() 

# ===== H√†m x·ª≠ l√Ω b·∫£ng V v√† alpha =====
def initialize_V():
    """Kh·ªüi t·∫°o I b·∫£ng V cho c√°c c·∫∑p (state, action)"""
    V_tables = [{} for _ in range(I)] #t·∫°o c√°c t·ª´ ƒëi·ªÉn r·ªóng
    return V_tables

def update_V(V, state, action):
    """C·∫≠p nh·∫≠t b·∫£ng V - ƒë·∫øm s·ªë l·∫ßn truy c·∫≠p (state, action)"""
    key = state_action_to_key(state, action)
    if key not in V:
        V[key] = 0
    V[key] += 1
    return V

def initialize_alpha():
    """Kh·ªüi t·∫°o I b·∫£ng alpha"""
    return [{} for _ in range(I)]

def update_alpha(alpha, V, state, action):
    """C·∫≠p nh·∫≠t b·∫£ng alpha - learning rate gi·∫£m theo s·ªë l·∫ßn truy c·∫≠p"""
    key = state_action_to_key(state, action)
    alpha[key] = 1.0 / V[key] if key in V and V[key] > 0 else 1.0 # anpha = 1 n·∫øu ch∆∞a truy c·∫≠p
    return alpha

# ===== L·ªõp qu·∫£n l√Ω Q Networks v·ªõi b·∫£ng V v√† alpha =====
# ===== C·∫£i ti·∫øn ƒë∆°n gi·∫£n cho QNetwork hi·ªán t·∫°i =====
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

class ImprovedQNetwork(nn.Module):
    def __init__(self):
        super(ImprovedQNetwork, self).__init__()
        self.state_dim = NUM_DEVICES * 4  # M·ªói thi·∫øt b·ªã c√≥ 4 ƒë·∫∑c tr∆∞ng
        self.action_size = 3**NUM_DEVICES  # T·ªïng s·ªë action (3 actions cho m·ªói thi·∫øt b·ªã)
        
        # X√¢y d·ª±ng m·∫°ng neural v·ªõi ki·∫øn tr√∫c t·ªët h∆°n
        self.fc1 = nn.Linear(self.state_dim, 256)  # TƒÉng t·ª´ 128 l√™n 256
        self.bn1 = nn.BatchNorm1d(256)             # Th√™m Batch Normalization
        self.dropout1 = nn.Dropout(0.2)           # Th√™m Dropout ƒë·ªÉ tr√°nh overfitting
        
        self.fc2 = nn.Linear(256, 128)             # TƒÉng t·ª´ 64 l√™n 128
        self.bn2 = nn.BatchNorm1d(128)             # Batch Normalization
        self.dropout2 = nn.Dropout(0.1)           # Dropout nh·∫π h∆°n
        
        # Th√™m m·ªôt layer ·∫©n n·ªØa ƒë·ªÉ tƒÉng kh·∫£ nƒÉng h·ªçc
        self.fc3 = nn.Linear(128, 64)
        self.bn3 = nn.BatchNorm1d(64)
        
        # Output layer
        self.fc4 = nn.Linear(64, self.action_size)
        
        # Kh·ªüi t·∫°o tr·ªçng s·ªë t·ªët h∆°n
        self._initialize_weights()
        
    def _initialize_weights(self):
        """Kh·ªüi t·∫°o tr·ªçng s·ªë theo Xavier/He initialization"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)  # Xavier initialization
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, state):
        """
        Nh·∫≠n ƒë·∫ßu v√†o l√† state, tr·∫£ v·ªÅ Q-values cho t·∫•t c·∫£ action kh·∫£ thi
        """
        # Chuy·ªÉn state th√†nh tensor v√† l√†m ph·∫≥ng
        if not isinstance(state, torch.Tensor):
            state = torch.FloatTensor(state.flatten()).unsqueeze(0)
        
        # Layer 1
        x = self.fc1(state)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout1(x)
        
        # Layer 2  
        x = self.fc2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout2(x)
        
        # Layer 3
        x = self.fc3(x)
        x = self.bn3(x)
        x = F.relu(x)
        
        # Output layer (kh√¥ng c√≥ activation ƒë·ªÉ c√≥ th·ªÉ output √¢m/d∆∞∆°ng)
        x = self.fc4(x)
        
        return x
    
    def get_q_value(self, state, action):
        """L·∫•y Q-value cho m·ªôt action c·ª• th·ªÉ"""
        with torch.no_grad():
            q_values = self(state)
            action_idx = action_to_index(action)
            return q_values[0, action_idx].item()

# ===== C·∫≠p nh·∫≠t QNetworkManager v·ªõi improvements =====
class ImprovedQNetworkManager:
    def __init__(self):
        # Kh·ªüi t·∫°o I m·∫°ng neural network
        self.q_networks = []
        self.optimizers = []
        
        for _ in range(I):
            network = ImprovedQNetwork()
            # S·ª≠ d·ª•ng AdamW thay v√¨ Adam (t·ªët h∆°n cho regularization)
            optimizer = optim.AdamW(
                network.parameters(), 
                lr=0.001,              # Learning rate th·∫•p h∆°n cho stability
                weight_decay=1e-4      # Weight decay cho regularization
            )
            self.q_networks.append(network)
            self.optimizers.append(optimizer)
        
        # Th√™m learning rate scheduler
        self.schedulers = []
        for optimizer in self.optimizers:
            scheduler = optim.lr_scheduler.StepLR(
                optimizer, 
                step_size=2000,        # Gi·∫£m learning rate m·ªói 2000 steps
                gamma=0.95             # Gi·∫£m 5% m·ªói l·∫ßn
            )
            self.schedulers.append(scheduler)
        
        # Kh·ªüi t·∫°o b·∫£ng V v√† alpha (gi·ªØ nguy√™n)
        self.V = initialize_V()
        self.alpha = initialize_alpha()
    
    def update_v_alpha(self, network_idx, state, action):
        """C·∫≠p nh·∫≠t b·∫£ng V v√† alpha cho c·∫∑p (state, action) - gi·ªØ nguy√™n"""
        self.V[network_idx] = update_V(self.V[network_idx], state, action)
        self.alpha[network_idx] = update_alpha(
            self.alpha[network_idx], 
            self.V[network_idx], 
            state, 
            action
        )
    
    def compute_risk_averse_Q(self, random_idx, state):
        """
        T√≠nh Q risk-averse cho t·∫•t c·∫£ action theo c√¥ng th·ª©c 22 - gi·ªØ nguy√™n logic
        """
        # Chuy·ªÉn state th√†nh tensor
        state_tensor = torch.FloatTensor(state.flatten()).unsqueeze(0)
        
        # L·∫•y Q-values t·ª´ m·∫°ng ƒë∆∞·ª£c ch·ªçn ng·∫´u nhi√™n H
        with torch.no_grad():
            q_random = self.q_networks[random_idx](state_tensor)
        
        # T√≠nh Q-values trung b√¨nh t·ª´ t·∫•t c·∫£ m·∫°ng
        q_avg = torch.zeros_like(q_random)
        for i in range(I):
            with torch.no_grad():
                q_avg += self.q_networks[i](state_tensor)
        q_avg /= I
        
        # T√≠nh t·ªïng b√¨nh ph∆∞∆°ng ƒë·ªô l·ªách
        sum_squared = torch.zeros_like(q_random)
        for i in range(I):
            with torch.no_grad():
                diff = self.q_networks[i](state_tensor) - q_avg
                sum_squared += diff * diff
        
        # T√≠nh Q risk-averse
        risk_term = -LAMBDA_P * sum_squared / (I - 1) if I > 1 else 0
        risk_averse_q = q_random + risk_term
        
        return risk_averse_q
    
    def choose_action(self, state, epsilon, H):
        """Ch·ªçn action theo epsilon-greedy v·ªõi Q risk-averse - gi·ªØ nguy√™n logic"""
        if random.random() < epsilon:
            return np.random.randint(0, 3, NUM_DEVICES)
        
        random_idx = H
        # T√≠nh Q risk-averse cho t·∫•t c·∫£ action
        risk_averse_q = self.compute_risk_averse_Q(random_idx, state)
        
        # Ch·ªçn action v·ªõi Q risk-averse cao nh·∫•t
        best_action_idx = torch.argmax(risk_averse_q).item()
        return index_to_action(best_action_idx)
    
    def update_q_network(self, network_idx, state, action, reward, next_state):
        """
        C·∫≠p nh·∫≠t Q-network theo c√¥ng th·ª©c 23 - c·∫£i ti·∫øn training process
        """
        # 1. C·∫≠p nh·∫≠t V v√† alpha
        self.update_v_alpha(network_idx, state, action)
        
        # 2. L·∫•y network v√† optimizer t∆∞∆°ng ·ª©ng
        network = self.q_networks[network_idx]
        optimizer = self.optimizers[network_idx]
        scheduler = self.schedulers[network_idx]
        
        # 3. Set network to training mode (quan tr·ªçng cho BatchNorm v√† Dropout)
        network.train()
        
        # 4. Chuy·ªÉn state th√†nh tensor
        state_tensor = torch.FloatTensor(state.flatten()).unsqueeze(0)
        next_state_tensor = torch.FloatTensor(next_state.flatten()).unsqueeze(0)
        
        # 5. T√≠nh Q-values hi·ªán t·∫°i
        q_values = network(state_tensor)
        action_idx = action_to_index(action)
        current_q = q_values[0, action_idx]
        
        # 6. T√≠nh max Q-value cho next state (set to eval mode ƒë·ªÉ t·∫Øt dropout)
        network.eval()
        with torch.no_grad():
            next_q_values = network(next_state_tensor)
            max_next_q = torch.max(next_q_values).item()
        network.train()  # Tr·ªü l·∫°i training mode
        
        # 7. T√≠nh TD error: r + Œ≥*max_a'Q(s',a') - Q(s,a)
        td_error = reward + GAMMA * max_next_q - current_q.item()
        
        # 8. T√≠nh utility: u(TD error) - x_0
        utility_value = u(td_error) - X0
        
        # 9. L·∫•y alpha t·ª´ b·∫£ng
        key = state_action_to_key(state, action)
        alpha_value = self.alpha[network_idx].get(key, 1.0)
        
        # 10. T·∫°o target Q-values
        target_q_values = q_values.clone().detach()
        target_q_values[0, action_idx] = current_q + alpha_value * utility_value
        
        # 11. T√≠nh loss v√† c·∫≠p nh·∫≠t network
        loss = F.mse_loss(q_values[0, action_idx:action_idx+1], 
                         target_q_values[0, action_idx:action_idx+1])
        
        # 12. Backpropagation v·ªõi gradient clipping
        optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping ƒë·ªÉ tr√°nh exploding gradients
        torch.nn.utils.clip_grad_norm_(network.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        # 13. C·∫≠p nh·∫≠t learning rate schedule
        scheduler.step()
        
        # 14. Set v·ªÅ eval mode sau khi training
        network.eval()

# ===== H√†m helper ƒë·ªÉ thay th·∫ø trong main code =====
def create_improved_q_manager():
    """T·∫°o improved Q manager - ch·ªâ c·∫ßn thay th·∫ø 1 d√≤ng trong main"""
    return ImprovedQNetworkManager()


# C√°c h√†m kh·ªüi t·∫°o v√† c·∫≠p nh·∫≠t state
def initialize_state():
    state = np.zeros(shape=(NUM_DEVICES, 4))
    return state

def update_state(state, plr, feedback):
    next_state = np.zeros(shape=(NUM_DEVICES, 4))
    for k in range(NUM_DEVICES):
        for i in range(2):
            if(plr[k, i] <= PLR_MAX):
                next_state[k, i] = 1
            elif(plr[k, i] > PLR_MAX):
                next_state[k, i] = 0
            next_state[k, i+2] = feedback[k, i]
    return next_state

# Kh·ªüi t·∫°o action
def initialize_action():
    action = np.random.randint(0, 3, NUM_DEVICES)
    return action

# C√°c h√†m kh√°c t·ª´ m√£ g·ªëc gi·ªØ nguy√™n
def create_h_base(num_of_frame, mean = 0, sigma = 1):
    h_base = []
    h_base_sub = env.generate_h_base(mean, sigma, num_of_frame*NUM_DEVICES*NUM_SUBCHANNELS)
    h_base_mW = env.generate_h_base(mean, sigma, num_of_frame*NUM_DEVICES*NUM_BEAMS)

    for frame in range(num_of_frame):
        h_base_sub_t = np.empty(shape=(NUM_DEVICES, NUM_SUBCHANNELS), dtype=complex)
        for k in range(NUM_DEVICES):
            for n in range(NUM_SUBCHANNELS):
                h_base_sub_t[k, n] = h_base_sub[frame*NUM_DEVICES*NUM_SUBCHANNELS + k*NUM_SUBCHANNELS + n]

        h_base_mW_t = np.empty(shape=(NUM_DEVICES, NUM_BEAMS), dtype=complex)
        for k in range(NUM_DEVICES):
            for n in range(NUM_BEAMS):
                h_base_mW_t[k, n] = h_base_mW[frame*NUM_DEVICES*NUM_BEAMS + k*NUM_BEAMS + n]
        h_base_t = [h_base_sub_t, h_base_mW_t]
        h_base.append(h_base_t)
    return h_base

def compute_r(device_positions, h_base, allocation, frame):
    r = []
    r_sub = np.zeros(NUM_DEVICES)
    r_mW = np.zeros(NUM_DEVICES)
    h_base_sub = h_base[0]

    for k in range(NUM_DEVICES):
        sub_channel_index = allocation[0][k]
        mW_beam_index = allocation[1][k]

        if(sub_channel_index != -1):
            h_sub_k = env.h_sub(device_positions, k, h_base_sub[k, sub_channel_index])
            r_sub[k] = env.r_sub(h_sub_k, device_index=k)
        if(mW_beam_index != -1):
            h_mW_k = env.h_mW(device_positions, k, frame)
            r_mW[k] = env.r_mW(h_mW_k, device_index=k)

    r.append(r_sub)
    r.append(r_mW)
    return r

def l_kv_success(r):
    l_kv_success = np.floor(np.multiply(r, T/D))
    return l_kv_success

def compute_average_rate(average_r, last_r, frame_num):
    avg_r = average_r.copy()
    for k in range(NUM_DEVICES):
        avg_r[0][k] = (last_r[0][k] + avg_r[0][k]*(frame_num - 1))/frame_num
        avg_r[1][k] = (last_r[1][k] + avg_r[1][k]*(frame_num - 1))/frame_num
    return avg_r

def allocate(action): #ph√¢n ph·ªëi t·ª´ action ƒë·∫øn c√°c ch√≠nh x√°c k√™nh c·ªßa t·ª´ng interface
    sub =[]
    mW = []
    for i in range(NUM_DEVICES):
        sub.append(-1)
        mW.append(-1)

    rand_sub = []
    rand_mW = []
    for i in range(NUM_SUBCHANNELS):
        rand_sub.append(i)
    for i in range(NUM_BEAMS):
        rand_mW.append(i)
        
    for k in range(NUM_DEVICES):
        if(action[k] == 0):
            if len(rand_sub) > 0:
                rand_index = np.random.randint(len(rand_sub))
                sub[k] = rand_sub[rand_index]
                rand_sub.pop(rand_index)
        if(action[k] == 1):
            if len(rand_mW) > 0:
                rand_index = np.random.randint(len(rand_mW))
                mW[k] = rand_mW[rand_index]
                rand_mW.pop(rand_index)
        if(action[k] == 2):
            if len(rand_sub) > 0 and len(rand_mW) > 0:
                rand_sub_index = np.random.randint(len(rand_sub))
                rand_mW_index = np.random.randint(len(rand_mW))
                sub[k] = rand_sub[rand_sub_index]
                mW[k] = rand_mW[rand_mW_index]
                rand_sub.pop(rand_sub_index)
                rand_mW.pop(rand_mW_index)
    
    allocate = [sub, mW]
    return allocate

def perform_action(action, l_sub_max, l_mW_max):
    number_of_packet = np.zeros(shape=(NUM_DEVICES, 2))
    for k in range(NUM_DEVICES):
        l_sub_max_k = l_sub_max[k]
        l_mW_max_k = l_mW_max[k]
        if(action[k] == 0):
            number_of_packet[k, 0] = min(l_sub_max_k, MAX_PACKETS)
            number_of_packet[k, 1] = 0
        if(action[k] == 1):
            number_of_packet[k, 0] = 0
            number_of_packet[k, 1] = min(l_mW_max_k, MAX_PACKETS)
        if(action[k] == 2):
            if(l_mW_max_k < MAX_PACKETS):
                number_of_packet[k, 1] = min(l_mW_max_k, MAX_PACKETS)
                number_of_packet[k, 0] = min(l_sub_max_k, MAX_PACKETS - number_of_packet[k, 1])
            else:
                number_of_packet[k, 1] = MAX_PACKETS - 1
                number_of_packet[k, 0] = 1
    return number_of_packet

def receive_feedback(packet_send, l_sub_max, l_mW_max):
    feedback = np.zeros(shape=(NUM_DEVICES, 2))
    for k in range(NUM_DEVICES):
        l_sub_k = packet_send[k, 0]
        l_mW_k = packet_send[k, 1]
        feedback[k, 0] = min(l_sub_k, l_sub_max[k])
        feedback[k, 1] = min(l_mW_k, l_mW_max[k])
    return feedback

def compute_packet_loss_rate(frame_num, old_packet_loss_rate, received_paket_num, sent_packet_num):
    plr = np.zeros(shape=(NUM_DEVICES, 2))
    for k in range(NUM_DEVICES):
        plr[k, 0] = env.packet_loss_rate(frame_num, old_packet_loss_rate[k, 0], received_paket_num[k, 0], sent_packet_num[k, 0])
        plr[k, 1] = env.packet_loss_rate(frame_num, old_packet_loss_rate[k, 1], received_paket_num[k, 1], sent_packet_num[k, 1])
    return plr

def compute_reward(state, num_of_send_packet, num_of_received_packet, old_reward_value, frame_num):
    sum = 0
    for k in range(NUM_DEVICES):
        state_k = state[k]
        numerator = num_of_received_packet[k, 0] + num_of_received_packet[k, 1] # t·ªïng s·ªë g√≥i tin nh·∫≠n ƒë∆∞·ª£c ·ªü UE
        denominator = num_of_send_packet[k, 0] + num_of_send_packet[k, 1] # t·ªïng s·ªë g√≥i tin g·ª≠i ƒëi t·ª´ AP

        if denominator == 0:
            success_rate_k = 0.0
        else:
            success_rate_k = numerator / denominator

        plr_penalty_sub = (1 - state_k[0])
        plr_penalty_mW = (1 - state_k[1])

        sum = sum + success_rate_k - plr_penalty_sub - plr_penalty_mW
    reward = ((frame_num - 1) * old_reward_value + sum) / frame_num
    return reward

# Ch∆∞∆°ng tr√¨nh ch√≠nh
if __name__ == "__main__":
    # Kh·ªüi t·∫°o manager cho c√°c Q-Networks
    q_manager = create_improved_q_manager()
    # Kh·ªüi t·∫°o m√¥i tr∆∞·ªùng
    device_positions = env.initialize_pos_of_devices()
    state = initialize_state()
    action = initialize_action()
    reward_value = 0.0
    allocation = allocate(action)
    packet_loss_rate = np.zeros(shape=(NUM_DEVICES, 2))
    
    # T·∫°o h_base cho m·ªói frame
    h_base = create_h_base(NUM_OF_FRAME + 1)
    h_base_t = h_base[0] #h·ªá s·ªë phai m·ªù k√™nh cho sub-6GHz
    average_r = compute_r(device_positions, h_base_t, allocation=allocate(action), frame=1)
    
    # C√°c bi·∫øn l∆∞u k·∫øt qu·∫£
    state_plot = []
    action_plot = []
    reward_plot = []
    packet_loss_rate_plot = []
    rate_plot = []
    number_of_received_packet_plot = []
    number_of_send_packet_plot = []
    # V√≤ng l·∫∑p ch√≠nh
    for frame in range(1, NUM_OF_FRAME + 1):
        # C·∫≠p nh·∫≠t epsilon
        EPSILON = EPSILON * LAMBDA
        
        # Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng
        h_base_t = h_base[frame]
        
        # Ch·ªçn ng·∫´u nhi√™n m·ªôt Q-network (t∆∞∆°ng ·ª©ng v·ªõi H trong m√£ g·ªëc)
        H = np.random.randint(0, I)
        
        # Ch·ªçn action s·ª≠ d·ª•ng Q risk-averse
        action = q_manager.choose_action(state, EPSILON, H)
        action_plot.append(action)

        allocation = allocate(action)
        
        # Th·ª±c hi·ªán action
        l_max_estimate = l_kv_success(average_r)
        l_sub_max_estimate = l_max_estimate[0]
        l_mW_max_estimate = l_max_estimate[1]

        number_of_send_packet = perform_action(action, l_sub_max_estimate, l_mW_max_estimate)
        number_of_send_packet_plot.append(number_of_send_packet)

        # Nh·∫≠n feedback
        r = compute_r(device_positions, h_base_t, allocation, frame)
        rate_plot.append(r)
        
        l_max = l_kv_success(r)
        l_sub_max = l_max[0]
        l_mW_max = l_max[1]
        
        number_of_received_packet = receive_feedback(number_of_send_packet, l_sub_max, l_mW_max)
        number_of_received_packet_plot.append(number_of_received_packet)

        packet_loss_rate = compute_packet_loss_rate(frame, packet_loss_rate, number_of_received_packet, number_of_send_packet)
        packet_loss_rate_plot.append(packet_loss_rate)
        
        average_r = compute_average_rate(average_r, r, frame)
        
        # T√≠nh reward
        reward_value = compute_reward(state, number_of_send_packet, number_of_received_packet, reward_value, frame)
        reward_plot.append(reward_value)
        
        next_state = update_state(state, packet_loss_rate, number_of_received_packet)
        
        # T·∫°o mask J (Poisson)
        J = np.random.poisson(1, I)
        
        # C·∫≠p nh·∫≠t c√°c Q-networks
        for i in range(I):
            if J[i] == 1:
                q_manager.update_q_network(i, state, action, reward_value, next_state)
        
        # Chuy·ªÉn sang tr·∫°ng th√°i m·ªõi
        state = next_state
        
        # In th√¥ng tin
        if frame % 100 == 0:
            print(f"Frame {frame}: Reward = {reward_value}")
            print(f"PLR = {packet_loss_rate}")
    

    #===Avg reward ====
    total_reward = np.sum(reward_plot)
    print("Avg reward:", total_reward/10000)
    # ==== Avg success =====
    total_received = sum(np.sum(arr) for arr in number_of_received_packet_plot)
    total_send = sum(np.sum(arr) for arr in number_of_send_packet_plot)
    print("Avg success:", total_received/total_send)
    # ==== PLR per device =====
    packet_loss_rate_plot = np.array(packet_loss_rate_plot)
    plr_sum_per_device = np.sum(packet_loss_rate_plot, axis=2) #plr c·ªßa t·ª´ng thi·∫øt b·ªã qua t·ª´ng frame 
    total_plr_per_device = np.sum(plr_sum_per_device, axis=0) #t·ªïng c·ªßa plr c·ªßa t·∫•t c·∫£ frames cho t·ª´ng thi·∫øt b·ªã (10000 frames)

    #t√≠nh trung b√¨nh plr c·ªßa t·ª´ng thi·∫øt b·ªã qua t·∫•t c·∫£ frames
    for i, total in enumerate(total_plr_per_device):
        avg_plr_of_devices = 0.0
        print(f"Thi·∫øt b·ªã {i + 1}: Avg packet loss rate = {total/NUM_OF_FRAME}")
        avg_plr_of_devices += PLR_MAX*NUM_OF_FRAME - total
    avg_plr_total_of_device = avg_plr_of_devices / (NUM_DEVICES*NUM_OF_FRAME) #gi√° tr·ªã trung b√¨nh l·ªói tr√™n t·∫•t c·∫£ c√°c thi·∫øt b·ªã

    print("Avg plr_total_of_device:", avg_plr_total_of_device)

    # tunable_parameters = {
    # 'h_base_sub6': h_base_t,
    # 'state': state_plot,
    # 'action': action_plot,
    # 'reward': reward_plot,
    # 'packet_loss_rate': packet_loss_rate_plot,
    # 'rate_plot': rate_plot,
    # 'number_of_received_packet': number_of_received_packet_plot,
    # 'number_of_send_packet': number_of_send_packet_plot,
    # 'Avg reward': total_reward/10000,
    # 'Avg success': total_received/total_send,
    # }

    # save.save_tunable_parameters_txt(I, NUM_DEVICES, tunable_parameters, save_dir='tunable_para_test_03')

    # # V·∫Ω ƒë·ªì th·ªã reward
    # plt.figure(figsize=(12, 6))
    # plt.plot(range(1, NUM_OF_FRAME + 1), reward_plot, label='Reward theo frame', color='green')
    # plt.title('Bi·ªÉu ƒë·ªì Reward theo t·ª´ng Frame')
    # plt.xlabel('Frame')
    # plt.ylabel('Reward')
    # plt.grid(True)
    # plt.legend()
    # plt.tight_layout()
    # plt.show()
    
    # # V·∫Ω ƒë·ªì th·ªã PLR
    # packet_loss_rate_plot = np.array(packet_loss_rate_plot)
    # frames = np.arange(1, packet_loss_rate_plot.shape[0] + 1)
    # plr_sum_per_device = np.sum(packet_loss_rate_plot, axis=2)

    
    # plt.figure(figsize=(12, 6))
    # for device_idx in range(NUM_DEVICES):
    #     # plt.plot(frames, packet_loss_rate_plot[:, device_idx, 0], label=f'Device {device_idx+1} - sub-6GHz')
    #     # plt.plot(frames, packet_loss_rate_plot[:, device_idx, 1], label=f'Device {device_idx+1} - mmWave')
    #     plt.plot(frames, plr_sum_per_device[:, device_idx], label=f'Device {device_idx+1}')
    
    # # üëâ Th√™m ƒë∆∞·ªùng chu·∫©n y = 0.1
    # plt.axhline(y=0.1, color='black', linestyle='--', linewidth=1.5, label='plr_max')

    # plt.title('T·ªâ l·ªá m·∫•t g√≥i tin (PLR) theo t·ª´ng Frame')
    # plt.xlabel('Frame')
    # plt.ylabel('PLR')
    # plt.grid(True)
    # plt.legend()
    # plt.tight_layout()
    # plt.show()

    # # ===== Bi·ªÉu ƒë·ªì t·ªâ l·ªá s·ª≠ d·ª•ng action cho t·ª´ng thi·∫øt b·ªã =====
    # # Gi·∫£ s·ª≠ action_plot l√† m·ªôt list ch·ª©a c√°c array shape (NUM_DEVICES,)
    # action_array = np.array(action_plot)  # shape: (num_frames, num_devices)
    # num_frames, num_devices = action_array.shape
    # # Chu·∫©n b·ªã m·∫£ng l∆∞u ph·∫ßn trƒÉm
    # # shape: (3 h√†nh ƒë·ªông, num_devices)
    # percentages = np.zeros((3, num_devices))  # 3 d√≤ng: 0, 1, 2

    # # T√≠nh ph·∫ßn trƒÉm cho t·ª´ng h√†nh ƒë·ªông theo t·ª´ng thi·∫øt b·ªã
    # for action in [0, 1, 2]:
    #     # ƒê·∫øm s·ªë l·∫ßn action xu·∫•t hi·ªán ·ªü t·ª´ng c·ªôt (thi·∫øt b·ªã)
    #     counts = np.sum(action_array == action, axis=0)
    #     percentages[action] = counts / num_frames * 100  # Chuy·ªÉn sang ph·∫ßn trƒÉm

    # labels = [f'Device {i+1}' for i in range(num_devices)]
    # x = np.arange(num_devices)  # V·ªã tr√≠ c·ªôt
    # width = 0.25  # ƒê·ªô r·ªông c·ªßa m·ªói nh√≥m c·ªôt

    # plt.figure(figsize=(10, 6))
    # plt.bar(x - width, percentages[0], width, label='sub-6GHz (0)', color='skyblue')
    # plt.bar(x,         percentages[1], width, label='mmWave (1)', color='orange')
    # plt.bar(x + width, percentages[2], width, label='C·∫£ hai (2)', color='green')

    # plt.ylabel('Ratio (%)')
    # plt.title('Interface usage distribution per device, scenario 1')
    # plt.xticks(x, labels)
    # plt.ylim(0, 100)
    # plt.legend()
    # plt.grid(True, axis='y', linestyle='--', alpha=0.7)
    # plt.tight_layout()
    # plt.show()
